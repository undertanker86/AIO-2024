{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nfrom torchsummary import summary\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:57:43.720860Z","iopub.execute_input":"2025-03-24T12:57:43.721072Z","iopub.status.idle":"2025-03-24T12:57:49.308137Z","shell.execute_reply.started":"2025-03-24T12:57:43.721050Z","shell.execute_reply":"2025-03-24T12:57:49.307423Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 256\nimg_size = 28\nchannels = 1\nlatent_dim = 10\nnum_epochs = 1\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n])\n\ndataset = datasets.MNIST(\n root=\"./data\", # Data storage directory\n train=True,\n transform=transform,\n download=True,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split data","metadata":{}},{"cell_type":"code","source":"train_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\nprint(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model VAE","metadata":{}},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, channels, latent_dim):\n        super(VAE, self).__init__()\n        # elements from encoder\n        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n\n        self.flatten_dim = 64 * 7 * 7\n        self.fc_mean = nn.Linear(self.flatten_dim, latent_dim)\n        self.fc_logvar = nn.Linear(self.flatten_dim, latent_dim)\n        # elements from decoder\n        self.fc_decode = nn.Linear(latent_dim, self.flatten_dim)\n        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.deconv2 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.conv_final = nn.Conv2d(16, channels, kernel_size=3, padding=1)\n\n    def encoder(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        \n        x = x.view(-1, self.flatten_dim)\n\n\n        mu = self.fc_mean(x)\n        logvar = self.fc_logvar(x)\n        return mu, logvar\n\n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n\n    def decode(self, z):\n        x = F.relu(self.fc_decode(z))\n        x = x.view(-1, 64, 7, 7)\n        x = F.relu(self.deconv1(x))\n        x = F.relu(self.deconv2(x))\n        x = torch.sigmoid(self.conv_final(x))\n        return x\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Losses","metadata":{}},{"cell_type":"code","source":"def loss_function(recon_x, x, mu, log_var, B=1000):\n    recon_x_flat = recon_x.view(recon_x.size(0), -1)\n    x_flat = x.view(x.size(0), -1)\n    mse_out = F.mse_loss(recon_x_flat, x_flat, reduction='mean')\n\n\n    reconstruction_loss = mse_out * x.shape[1] * x.shape[2] * x.shape[3]\n\n    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), axis=1)\n\n    total_loss = B * reconstruction_loss + torch.mean(kl_loss)\n\n    return total_loss, reconstruction_loss, torch.mean(kl_loss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = VAE(channels, latent_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\n# Create a log directory if it doesn't exist\nlog_dir = \"logs\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Create a log file with timestamp\ntimestamp = datetime.now().strftime(\"%Y/%m/%d_%H:%M:%S\")\nlog_file = os.path.join(log_dir, f'training_log_{timestamp}.txt')\n\n# Open the log file\nwith open(log_file, \"w\") as f:\n    f.write(f\"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    f.write(f\"Model: VAE with latent_dim={latent_dim}\\n\")\n    f.write(f\"Batch size: {batch_size}, Image size: {img_size}x{img_size}\\n\")\n    f.write(f\"Total epochs: {num_epochs}\\n\\n\")\n    f.write(f\"Epoch,Avg_Loss,Recon_Loss,KL_Loss\\n\")\n\n# Training loop with logging\nmodel.train()\nfor epoch in range(num_epochs):\n    train_loss = 0.0\n    epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n    for data, _ in epoch_bar:\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        epoch_bar.set_postfix(loss=loss.item())\n\n# Calculate average loss\navg_loss = train_loss / len(train_loader.dataset)\n# Print epoch summary\nprint(f\"Epoch {epoch+1}/{num_epochs} Loss per sample: {avg_loss:.4f} \"\n      f\"Recon Loss: {recon_loss.item():.4f} KL Loss: {kl_loss.item():.4f}\")\n\n# Save to log file\nwith open(log_file, \"a\") as f:\n    f.write(f\"{epoch+1},{avg_loss:.6f},{recon_loss.item():.6f},{kl_loss.item():.6f}\\n\")\n\n# Log training completion\nwith open(log_file, \"a\") as f:\n    f.write(f\"\\nTraining completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\nprint(f\"Training log saved to {log_file}\")\n# After training, visualize the reconstruction on validation images\nmodel.eval()\nwith torch.no_grad():\n    data_iter = iter(val_loader)\n    images, _ = next(data_iter)\n    images = images.to(device)\n    recon_images, _, _ = model(images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot original and reconstructed images side by side\nn = 10  # number of images to display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Original image: (C,H,W) -> (H,W,C)\n    orig = images[i].cpu().permute(1, 2, 0).numpy()\n    recon = recon_images[i].cpu().permute(1, 2, 0).numpy()\n\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(orig)\n    plt.title(\"Original\")\n    plt.axis('off')\n\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(recon)\n    plt.title(\"Reconstructed\")\n    plt.axis('off')\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}